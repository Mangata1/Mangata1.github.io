<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning | Mangata Blog</title><meta name="keywords" content="KG,policy-based RL"><meta name="author" content="rm-rf /*"><meta name="copyright" content="rm-rf /*"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Chinese Version OX00 AbstractWe study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning mult">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning">
<meta property="og:url" content="http://mangata.tulip.academy/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/index.html">
<meta property="og:site_name" content="Mangata Blog">
<meta property="og:description" content="Chinese Version OX00 AbstractWe study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning mult">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://mangata.tulip.academy/img/back/18.jpg">
<meta property="article:published_time" content="2021-05-09T07:17:45.000Z">
<meta property="article:modified_time" content="2021-05-10T07:17:02.023Z">
<meta property="article:author" content="rm-rf &#x2F;*">
<meta property="article:tag" content="KG">
<meta property="article:tag" content="policy-based RL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://mangata.tulip.academy/img/back/18.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://mangata.tulip.academy/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-right"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-10 15:17:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> MessageBoard</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/back/18.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Mangata Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> MessageBoard</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-09T07:17:45.000Z" title="发表于 2021-05-09 15:17:45">2021-05-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-05-10T07:17:02.023Z" title="更新于 2021-05-10 15:17:02">2021-05-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97825216"><strong>Chinese Version</strong></a></p>
<h2 id="OX00-Abstract"><a href="#OX00-Abstract" class="headerlink" title="OX00 Abstract"></a>OX00 Abstract</h2><p>We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: <strong>we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.</strong> In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.</p>
<h2 id="OX01-Introduction"><a href="#OX01-Introduction" class="headerlink" title="OX01 Introduction"></a>OX01 Introduction</h2><p>the Path-Ranking Algorithm (PRA) (Lao et al., 2010, 2011a) emerges as a promising method for learning inference paths in large KGs. PRA uses a random-walk with restarts based inference mechanism to perform multiple bounded depth-first search processes to find relational paths. Coupled with elastic-net based learning, PRA then picks more plausible paths using supervised learning. <strong>However, PRA operates in a fully discrete space, which makes it difficult to evaluate and compare similar entities and relations in a KG.</strong></p>
<p>In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL). In contrast to PRA, we use translationbased knowledge based embedding method (Bordes et al., 2013) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph. <strong>The agent takes incremental steps by sampling a relation to extend its path.</strong> To better guide the RL agent for learning relational paths, we use policy gradient training (<strong>Mnih et al., 2015</strong>) with a novel reward function that jointly encourages <strong>accuracy, diversity, and efficiency.</strong> Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language Learning (Carlson et al., 2010a) dataset. Our contributions are three-fold: </p>
<ul>
<li><p> We are the first to consider reinforcement learning (RL) methods for learning relational paths in knowledge graphs;</p>
</li>
<li><p>Our learning method uses a complex reward function that considers accuracy, efficiency, and path diversity simultaneously, offering better control and more flexibility in the path finding process; </p>
</li>
<li><p>We show that our method can scale up to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks.</p>
</li>
</ul>
<h2 id="OX02-Methodlogy"><a href="#OX02-Methodlogy" class="headerlink" title="OX02 Methodlogy"></a>OX02 Methodlogy</h2><p>The specific task of relation reasoning is to find reliable predictive paths between entity pairs. <strong>We formulate the path finding problem as a sequential decision making problem which can be solved with a RL agent.</strong></p>
<h3 id="1-Reinforcement-Learning-for-Relation-Reasoning"><a href="#1-Reinforcement-Learning-for-Relation-Reasoning" class="headerlink" title="1. Reinforcement Learning for Relation Reasoning"></a>1. Reinforcement Learning for Relation Reasoning</h3><p>The RL system consists of two parts (see Figure 1). The first part is the external environment E which specifies the dynamics of the interaction between the agent and the KG. This environment is modeled as a Markov decision process (MDP).</p>
<p><img src="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/1.png" alt="RL Model"></p>
<p>The second part of the system, the RL agent, is represented as a <strong>policy network</strong>  which maps the state vector to a stochastic policy. The neural network parameters θ are updated using stochastic gradient descent. <strong>Compared to Deep Q Network (DQN) (Mnih et al., 2013), policy-based RL methods turn out to be more appropriate for our knowledge graph scenario. One reason is that for the path finding problem in KG, the action space can be very large due to complexity of the relation graph. This can lead to poor convergence properties for DQN. Besides, instead of learning a greedy policy which is common in value-based methods like DQN, the policy network is able to learn a stochastic policy which prevent the agent from getting stuck at an intermediate state.</strong></p>
<p><strong>Actions</strong>   <strong>Given the entity pairs $(e_s, e_t)$ with relation r, we want the agent to find the most informative paths linking these entity pairs. Beginning with the source entity $e_s$, the agent use the policy network to pick the most promising relation to extend its path at each step until it reaches the target entity et .</strong> <em>To keep the output dimension of the policy network consistent, the action space is defined as all the relations in the KG</em>.</p>
<p><strong>States</strong>   The entities and relations in a KG are naturally discrete atomic symbols. Since existing practical KGs like Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010b) often have huge amounts of triples. It is impossible to directly model all the symbolic atoms in states. To capture the semantic information of these symbols, we use translation-based embeddings such as TransE (Bordes et al., 2013) and TransH (Wang et al., 2014) to represent the entities and relations. These embeddings map all the symbols to a low dimensional vector space. In our framework, each state captures the agent’s position in the KG. After taking an action, the agent will move from one entity to another. These two are linked by the action (relation) just taken by the agent. The state vector at step t is given as follows: $s_t = (e_t , e_{target} − e_t) $ where $e_t$ denotes the embeddings of the current entity node and etarget denotes the embeddings of the target entity. At the initial state, et = esource. <strong>We do not incorporate the reasoning relation in the state, because the embedding of the reasoning relation remain constant during path finding, which is not helpful in training. However, we find out that by training the RL agent using a set of positive samples for one particular relation, the agent can successfully discover the relation semantics.</strong></p>
<p> <strong>Rewards</strong>   There are a few factors that contribute to the quality of the paths found by the RL agent. To encourage the agent to find predictive paths, our reward functions include the following scoring criteria: </p>
<ul>
<li><p><strong>Global accuracy</strong>:   For our environment settings, the number of actions that can be taken by the agent can be very large. In other words, <strong>there are much more incorrect sequential decisions than the correct ones. The number of these incorrect decision sequences can increase exponentially with the length of the path.</strong> In view of this challenge, the first reward function we add to the RL model is defined as follows:<br>$$<br>r_{GLOBAL}=\left{<br>\begin{aligned}<br>+1 &amp;  &amp; if \ the \ path \ reaches\  etarget \<br>−1 &amp; &amp; otherwise \<br>\end{aligned}<br>\right.<br>$$<br>the agent is given an offline positive reward +1 if it reaches the target after a sequence of actions. </p>
</li>
<li><p><strong>Path efficiency</strong>:    For the relation reasoning task, we observe that short paths tend to provide more reliable reasoning evidence than longer paths. Shorter chains of relations can also improve the efficiency of the reasoning by limiting the length of the RL’s interactions with the environment. The efficiency reward is defined as follows:<br>$$<br>r_{EFFICIENCY} = \frac{1}  {length(p)}<br>$$<br>where path p is defined as a sequence of relations r1 → r2 → … → rn.</p>
<p> <strong>Path diversity</strong>: We train the agent to find paths using positive samples for each relation. These training sample $(e_{source}, e_{target})$  have similar state representations in the vector space. <strong>The agent tends to find paths with similar syntax and semantics. These paths often contains redundant information since some of them may be correlated.</strong> To encourage the agent to find diverse paths, we define a diversity reward function using the cosine similarity between the current path and the existing ones: <img src="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/2.png" alt="math"></p>
<p>represents the path embedding for the relation chain r1 → r2 → … → rn. </p>
<p><strong>Policy Network</strong>  <strong>We use a fully-connected neural network to parameterize the policy function</strong> $π(s; θ)$ that <strong>maps the state vector s to a probability distribution over all possible actions</strong>. The neural network consists of two hidden layers, each followed by a rectifier nonlinearity layer (ReLU). The output layer is normalized using a softmax function (see Figure 1).</p>
</li>
</ul>
<h3 id="2-Training-Pipeline"><a href="#2-Training-Pipeline" class="headerlink" title="2. Training Pipeline"></a>2. Training Pipeline</h3><p>In practice, one big challenge of KG reasoning is that the relation set can be quite large. <strong>For a typical KG, the RL agent is often faced with hundreds (thousands) of possible actions. In other words, the output layer of the policy network often has a large dimension. Due to the complexity of the relation graph and the large action space, if we directly train the RL model by trial and errors, which is typical for RL algorithms, the RL model will show very poor convergence properties.</strong> After a long-time training, the agents fails to find any valuable path. To tackle this problem, we start our training with a supervised policy which is inspired by the imitation learning pipeline used by AlphaGo (Silver et al., 2016). In the Go game, the player is facing nearly 250 possible legal moves at each step. Directly training the agent to pick actions from the original action space can be a difficult task. AlphaGo first train a supervised policy network using experts moves. In our case, the supervised policy is trained with <strong>a randomized breadth-first search (BFS).</strong> </p>
<p><strong>Supervised Policy Learning</strong>   For each relation, we use a subset of all the positive samples (entity pairs) to learn the supervised policy. For each positive sample (esource, etarget), a two-side BFS is conducted to find same correct paths between the entities. For each path p with a sequence of relations r1 → r2 → … → rn, we update the parameters θ to maximize the expected cumulative reward using Monte-Carlo Policy Gradient (REINFORCE) (Williams, 1992):</p>
<p><img src="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/3.png" alt="image-20210510102038681"></p>
<h3 id="3-Bi-directional-Path-constrained-Search"><a href="#3-Bi-directional-Path-constrained-Search" class="headerlink" title="3.  Bi-directional Path-constrained Search"></a>3.  Bi-directional Path-constrained Search</h3><p><strong>Given an entity pair, the reasoning paths learned by the RL agent can be used as logical formulas to predict the relation link.</strong> Each formula is verified using a bi-directional search. In a typical KG, one entity node can be linked to a large number of neighbors with the same relation link. A simple example is the relation ${personNationality}^{−1}$ , which denotes the inverse of $personNationality$. Following this link, the entity United States can reach numerous neighboring entities. If the formula consists of such links, the number of intermediate entities can exponentially increase as we follow the reasoning formula. However, we observe that for these formulas, if we verify the formula from the inverse direction. The number of intermediate nodes can be tremendously decreased. Algorithm 2 shows a detailed description of the proposed bi-directional search.</p>
<p><img src="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/6.png" alt="image-20210510151129976"></p>
<h2 id="OX04-Experiment"><a href="#OX04-Experiment" class="headerlink" title="OX04 Experiment"></a>OX04 Experiment</h2><p>To evaluate the reasoning formulas found by our RL agent, we explore two standard KG reasoning tasks: l<strong>ink prediction (predicting target entities) and fact prediction (predicting whether an unknown fact holds or not)</strong>. We compare our method with both path-based methods and embedding based methods. After that, we further analyze the reasoning paths found by our RL agent. These highly predictive paths validate the effectiveness of the reward functions. Finally, we conduct a experiment to investigate the effect of the supervised learning procedure.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">rm-rf /*</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://mangata.tulip.academy/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/">http://mangata.tulip.academy/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Mangata.tulip.academy" target="_blank">Mangata Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/KG/">KG</a><a class="post-meta__tags" href="/tags/policy-based-RL/">policy-based RL</a></div><div class="post_share"><div class="social-share" data-image="/img/back/18.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/Inkedreward_LI.jpg" target="_blank"><img class="post-qr-code-img" src="/img/Inkedreward_LI.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/06/01/Advances-and-Open-Problems-in-Federated-Learning/"><img class="prev-cover" src="/img/back/20.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Advances and Open Problems in Federated Learning</div></div></a></div><div class="next-post pull-right"><a href="/2021/05/06/Latex-Use-Introduction/"><img class="next-cover" src="/img/back/5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Latex Use Introduction</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">rm-rf /*</div><div class="author-info__description">最大似然估计：所有已经发生的，都是最应该发生的</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Mangata1"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">contract author :You can leave a message on the messageboard, I will check it!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#OX00-Abstract"><span class="toc-number">1.</span> <span class="toc-text">OX00 Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OX01-Introduction"><span class="toc-number">2.</span> <span class="toc-text">OX01 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OX02-Methodlogy"><span class="toc-number">3.</span> <span class="toc-text">OX02 Methodlogy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Reinforcement-Learning-for-Relation-Reasoning"><span class="toc-number">3.1.</span> <span class="toc-text">1. Reinforcement Learning for Relation Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Training-Pipeline"><span class="toc-number">3.2.</span> <span class="toc-text">2. Training Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Bi-directional-Path-constrained-Search"><span class="toc-number">3.3.</span> <span class="toc-text">3.  Bi-directional Path-constrained Search</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OX04-Experiment"><span class="toc-number">4.</span> <span class="toc-text">OX04 Experiment</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/02/DP-oVML/" title="DP-oVML"><img src="/img/back/16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DP-oVML"/></a><div class="content"><a class="title" href="/2021/09/02/DP-oVML/" title="DP-oVML">DP-oVML</a><time datetime="2021-09-02T15:09:52.000Z" title="发表于 2021-09-02 23:09:52">2021-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/02/Practical-Secure-Aggregation-for-Privacy-Preserving-Machine-Learning/" title="Practical Secure Aggregation for Privacy-Preserving Machine Learning"><img src="/img/back/6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Practical Secure Aggregation for Privacy-Preserving Machine Learning"/></a><div class="content"><a class="title" href="/2021/09/02/Practical-Secure-Aggregation-for-Privacy-Preserving-Machine-Learning/" title="Practical Secure Aggregation for Privacy-Preserving Machine Learning">Practical Secure Aggregation for Privacy-Preserving Machine Learning</a><time datetime="2021-09-02T14:40:30.183Z" title="发表于 2021-09-02 22:40:30">2021-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/06/01/Advances-and-Open-Problems-in-Federated-Learning/" title="Advances and Open Problems in Federated Learning"><img src="/img/back/20.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Advances and Open Problems in Federated Learning"/></a><div class="content"><a class="title" href="/2021/06/01/Advances-and-Open-Problems-in-Federated-Learning/" title="Advances and Open Problems in Federated Learning">Advances and Open Problems in Federated Learning</a><time datetime="2021-06-01T12:47:44.000Z" title="发表于 2021-06-01 20:47:44">2021-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/" title="DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning"><img src="/img/back/18.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning"/></a><div class="content"><a class="title" href="/2021/05/09/DeepPath-A-Reforcement-Learning-Method-for-Knowledge-Graph-Reasoning/" title="DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</a><time datetime="2021-05-09T07:17:45.000Z" title="发表于 2021-05-09 15:17:45">2021-05-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/05/06/Latex-Use-Introduction/" title="Latex Use Introduction"><img src="/img/back/5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Latex Use Introduction"/></a><div class="content"><a class="title" href="/2021/05/06/Latex-Use-Introduction/" title="Latex Use Introduction">Latex Use Introduction</a><time datetime="2021-05-06T14:35:06.000Z" title="发表于 2021-05-06 22:35:06">2021-05-06</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/back/18.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 By rm-rf /*</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://github.com/Mangata1">Mangata</a><span class="footer-separator">| rm-rf/*</span></div><div class="footer_custom_text">Hi welcome to my blog</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'DFUYeodUdvR7okd79UcGgqdI-gzGzoHsz',
      appKey: 'SJ5iMHkmpY4kEQ4LHpI9LprE',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: true,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: true,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script type="text/javascript" src="/js/fairyDustCursor.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>